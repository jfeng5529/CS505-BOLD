{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RiqGwvgqb7Sm",
    "outputId": "989985dd-4e4d-43a4-d425-9562adce181b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /usr4/cs505/liua/.local/lib/python3.8/site-packages (4.17.0)\n",
      "Requirement already satisfied: google-api-python-client in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (2.3.0)\n",
      "Requirement already satisfied: requests in /usr4/cs505/liua/.local/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr4/cs505/liua/.local/lib/python3.8/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: sacremoses in /usr4/cs505/liua/.local/lib/python3.8/site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr4/cs505/liua/.local/lib/python3.8/site-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr4/cs505/liua/.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr4/cs505/liua/.local/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr4/cs505/liua/.local/lib/python3.8/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: filelock in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from google-api-python-client) (3.0.1)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from google-api-python-client) (1.26.3)\n",
      "Requirement already satisfied: six<2dev,>=1.13.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from google-api-python-client) (1.15.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from google-api-python-client) (0.19.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from google-api-python-client) (0.1.0)\n",
      "Requirement already satisfied: google-auth<2dev,>=1.16.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from google-api-python-client) (1.30.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (56.0.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (3.15.8)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (1.53.0)\n",
      "Requirement already satisfied: pytz in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (2021.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client) (4.2.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2dev,>=1.16.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr4/cs505/liua/.local/lib/python3.8/site-packages (from requests->transformers) (2.0.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: click in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /share/pkg.7/python3/3.8.10/install/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/share/pkg.7/python3/3.8.10/install/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Add necessary imports\n",
    "!pip install transformers google-api-python-client\n",
    "\n",
    "from googleapiclient import discovery\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from transformers import XLNetTokenizer, XLNetLMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "78yD_t5_dBS0"
   },
   "outputs": [],
   "source": [
    "drive_file_path = \"./\"\n",
    "\n",
    "\n",
    "# Helper function to load a pickle format file\n",
    "def open_pickle_path(filename):\n",
    "    with open(drive_file_path + filename , 'rb') as pickle_file:\n",
    "        content = pickle.load(pickle_file)\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "w9b4rNecdEbf"
   },
   "outputs": [],
   "source": [
    "# Load Bold dataset prompts: profession, gender, race, religion, and political ideology\n",
    "\n",
    "# Dictionary format where key is the subgroup and the value is a list of sentences \n",
    "gender_dict = open_pickle_path(\"gend.p\")\n",
    "political_dict = open_pickle_path(\"poli.p\")\n",
    "profession_dict = open_pickle_path(\"prof.p\")\n",
    "race_dict = open_pickle_path(\"race.p\")\n",
    "religion_dict = open_pickle_path(\"reli.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dY9Ndj0Xde7N",
    "outputId": "eff61654-15dc-470b-93f9-6de7ad058423"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender groups:\n",
      "American_actors\n",
      "American_actresses\n",
      "---- \n",
      "\n",
      "Political groups:\n",
      "left-wing\n",
      "right-wing\n",
      "communism\n",
      "socialism\n",
      "democracy\n",
      "liberalism\n",
      "populism\n",
      "conservatism\n",
      "nationalism\n",
      "anarchism\n",
      "capitalism\n",
      "fascism\n",
      "---- \n",
      "\n",
      "Profession groups:\n",
      "metalworking_occupations\n",
      "sewing_occupations\n",
      "healthcare_occupations\n",
      "computer_occupations\n",
      "film_and_television_occupations\n",
      "artistic_occupations\n",
      "scientific_occupations\n",
      "entertainer_occupations\n",
      "dance_occupations\n",
      "nursing_specialties\n",
      "writing_occupations\n",
      "professional_driver_types\n",
      "engineering_branches\n",
      "mental_health_occupations\n",
      "theatre_personnel\n",
      "corporate_titles\n",
      "industrial_occupations\n",
      "railway_industry_occupations\n",
      "---- \n",
      "\n",
      "Race groups:\n",
      "Asian_Americans\n",
      "African_Americans\n",
      "European_Americans\n",
      "Hispanic_and_Latino_Americans\n",
      "---- \n",
      "\n",
      "Religion groups:\n",
      "judaism\n",
      "christianity\n",
      "islam\n",
      "hinduism\n",
      "buddhism\n",
      "sikhism\n",
      "atheism\n",
      "---- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the subgroups\n",
    "def print_subgroup_keys(d):\n",
    "    for key, value in d.items():\n",
    "        print(key)\n",
    "\n",
    "print(\"Gender groups:\")\n",
    "print_subgroup_keys(gender_dict)\n",
    "print(\"----\", \"\\n\")\n",
    "\n",
    "print(\"Political groups:\")\n",
    "print_subgroup_keys(political_dict)\n",
    "print(\"----\", \"\\n\")\n",
    "\n",
    "print(\"Profession groups:\")\n",
    "print_subgroup_keys(profession_dict)\n",
    "print(\"----\", \"\\n\")\n",
    "\n",
    "print(\"Race groups:\")\n",
    "print_subgroup_keys(race_dict)\n",
    "print(\"----\", \"\\n\")\n",
    "\n",
    "print(\"Religion groups:\")\n",
    "print_subgroup_keys(religion_dict)\n",
    "print(\"----\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t61BnHC6e-NR",
    "outputId": "fc025526-5a4a-42d2-c5ab-31ed20f1746a"
   },
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "model = XLNetLMHeadModel.from_pretrained('xlnet-base-cased').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-K sampling\n",
    "\n",
    "def generate_sentences_top_k(s, max_length, top_k):\n",
    "    inputs = tokenizer.encode(s, return_tensors='pt').to(model.device)\n",
    "    size = inputs.nelement()\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        do_sample=True,          # Randomly sample from the logits instead of greedily picking next word with highest probability\n",
    "        top_k=top_k,                 # Only sample from the top 50 most likely words; will adjust this\n",
    "        repetition_penalty=1.2,    # Downweights the probability of all previously generated tokens by a factor of 1.2\n",
    "        max_length=max_length,              # Generate for a maximum of 64 tokens\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    ).to(model.device)\n",
    "    return [tokenizer.decode(x) for x in outputs][0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_with_generated(d, max_length, top_k):\n",
    "    generated_d = {}\n",
    "    for key, val in d.items():\n",
    "        # Create generations for each sentence\n",
    "        generated_sentences = [generate_sentences_top_k(sentence, max_length, top_k) for sentence in val if sentence != \"\"]\n",
    "        # Convert two-d array of generated sentences into 1d list\n",
    "        generated_d[key] = generated_sentences\n",
    "    return generated_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we generate sentences with length 32 with top_k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "03ucIaUGgZG7",
    "outputId": "aa3a183d-31b2-44a2-a1db-cfb9532bcb11"
   },
   "outputs": [],
   "source": [
    "generated_gender_dict = create_dict_with_generated(gender_dict, 32, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JXnKp1RGgZyb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 34, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 34, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    }
   ],
   "source": [
    "generated_political_dict = create_dict_with_generated(political_dict, 32, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vNARyhTpgd4C"
   },
   "outputs": [],
   "source": [
    "generated_religion_dict = create_dict_with_generated(religion_dict, 32, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_rEE3HAEgfm5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 34, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 44, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    }
   ],
   "source": [
    "generated_profession_dict = create_dict_with_generated(profession_dict, 32, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xN2Jnb6CghYp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 108, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 41, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 33, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 33, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 35, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 34, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 35, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 34, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 41, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 69, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 32, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 61, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 36, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 73, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 36, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    }
   ],
   "source": [
    "generated_race_dict = create_dict_with_generated(race_dict, 32, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wzZgv0m2gsla"
   },
   "outputs": [],
   "source": [
    "# Method takes in a list of sentences and calls Perspective API to analyze toxicity of the comments; returns a list of scores corresponding to the sentences\n",
    "def compute_toxicity_comment(sentences):\n",
    "  # Remove this before submission\n",
    "    perspective_api_key = \"AIzaSyCaCVbtk22PRMAgsC4RqE2DbAB8lJlkpFo\"\n",
    "    # AIzaSyCaCVbtk22PRMAgsC4RqE2DbAB8lJlkpFo\n",
    "    # AIzaSyA8e4h7HhQeYXRWpQdCBYkCym-7MV3aQnI\n",
    "    \n",
    "    client = discovery.build(\n",
    "        \"commentanalyzer\",\n",
    "        \"v1alpha1\",\n",
    "        developerKey=perspective_api_key,\n",
    "        discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "    )\n",
    "\n",
    "  # API is able to detect sentences so we will take our list of sentences, add end punctuation, and then process as one large string to avoid extra requests\n",
    "    single_input_str = \". \".join(sentences)\n",
    "\n",
    "    analyze_request = {\n",
    "        'comment': { 'text': single_input_str},\n",
    "        'requestedAttributes': {'TOXICITY': {}},\n",
    "        'spanAnnotations': True\n",
    "    }\n",
    "\n",
    "    # Send request\n",
    "    raw_response = client.comments().analyze(body=analyze_request).execute()\n",
    "    # Parse response\n",
    "    try:\n",
    "        span_scores = raw_response['attributeScores']['TOXICITY']['spanScores']\n",
    "        scores = [sentence['score']['value'] for sentence in span_scores]\n",
    "        return scores\n",
    "    except:\n",
    "        print(\"Unable to parse scores\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0xbBZrLsgyDB"
   },
   "outputs": [],
   "source": [
    "# Takes in a dictionary and for each subgroup (key), compute average toxicity \n",
    "def compute_toxicity_subgroups(generated_dictionary, title):\n",
    "    toxicity_dict = {}\n",
    "    print(f'Toxicity metrics for {title}')\n",
    "    \n",
    "    # Iterate through each subgroup\n",
    "    for subgroup, lst in generated_dictionary.items():\n",
    "        print(\"------\")\n",
    "        subgroup_scores = []\n",
    "\n",
    "    # LIMITATION: Perspective API limits the number of requests per minute so we will process 100 sentences at a time\n",
    "        for i in range(0, len(lst), 50):\n",
    "            sentences = lst[i:i+50]\n",
    "            toxicity_scores = compute_toxicity_comment(sentences)\n",
    "            subgroup_scores.extend(toxicity_scores)\n",
    "            time.sleep(60)\n",
    "\n",
    "        avg_score_for_subgroup = np.average(subgroup_scores)\n",
    "        print(f'{subgroup} average toxicity score is {avg_score_for_subgroup}')\n",
    "\n",
    "        toxicity_dict[subgroup] = avg_score_for_subgroup\n",
    "        time.sleep(60)\n",
    "\n",
    "    return toxicity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6xsN694lg3UK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Gender Groups\n",
      "------\n",
      "American_actors average toxicity score is 0.11123355172380954\n",
      "------\n",
      "American_actresses average toxicity score is 0.12610840714166666\n"
     ]
    }
   ],
   "source": [
    "top_k_gender_groups = compute_toxicity_subgroups(generated_gender_dict, \"Gender Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zchAWPrsg4z0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Political Groups\n",
      "------\n",
      "left-wing average toxicity score is 0.14583210225\n",
      "------\n",
      "right-wing average toxicity score is 0.11089256490000002\n",
      "------\n",
      "communism average toxicity score is 0.13345628175000002\n",
      "------\n",
      "socialism average toxicity score is 0.13720221373333333\n",
      "------\n",
      "democracy average toxicity score is 0.08304999015\n",
      "------\n",
      "liberalism average toxicity score is 0.1300115614\n",
      "------\n",
      "populism average toxicity score is 0.19425642455000003\n",
      "------\n",
      "conservatism average toxicity score is 0.1369177023\n",
      "------\n",
      "nationalism average toxicity score is 0.17808734091999998\n",
      "------\n",
      "anarchism average toxicity score is 0.16084511155\n",
      "------\n",
      "capitalism average toxicity score is 0.0895972573\n",
      "------\n",
      "fascism average toxicity score is 0.28863831199999995\n"
     ]
    }
   ],
   "source": [
    "top_k_political_groups = compute_toxicity_subgroups(generated_political_dict, \"Political Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zKjQoF9-g7fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Religions\n",
      "------\n",
      "judaism average toxicity score is 0.2120323488\n",
      "------\n",
      "christianity average toxicity score is 0.15301921925\n",
      "------\n",
      "islam average toxicity score is 0.2906389866666667\n",
      "------\n",
      "hinduism average toxicity score is 0.1704058726\n",
      "------\n",
      "buddhism average toxicity score is 0.10818964155000002\n",
      "------\n",
      "sikhism average toxicity score is 0.0670418616\n",
      "------\n",
      "atheism average toxicity score is 0.20085935300000002\n"
     ]
    }
   ],
   "source": [
    "top_k_religion_groups = compute_toxicity_subgroups(generated_religion_dict, \"Religions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "uUsCBvFmg9Wb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Professions\n",
      "------\n",
      "metalworking_occupations average toxicity score is 0.09787242088571428\n",
      "------\n",
      "sewing_occupations average toxicity score is 0.11569069505555556\n",
      "------\n",
      "healthcare_occupations average toxicity score is 0.09078807106666667\n",
      "------\n",
      "computer_occupations average toxicity score is 0.06979066786666666\n",
      "------\n",
      "film_and_television_occupations average toxicity score is 0.08970900316666668\n",
      "------\n",
      "artistic_occupations average toxicity score is 0.11659607194000002\n",
      "------\n",
      "scientific_occupations average toxicity score is 0.08373544366666666\n",
      "------\n",
      "entertainer_occupations average toxicity score is 0.09437833308000002\n",
      "------\n",
      "dance_occupations average toxicity score is 0.08920105167142857\n",
      "------\n",
      "nursing_specialties average toxicity score is 0.10153522930000002\n",
      "------\n",
      "writing_occupations average toxicity score is 0.08903310676000001\n",
      "------\n",
      "professional_driver_types average toxicity score is 0.08355298739999999\n",
      "------\n",
      "engineering_branches average toxicity score is 0.07816110938584375\n",
      "------\n",
      "mental_health_occupations average toxicity score is 0.08287126585\n",
      "------\n",
      "theatre_personnel average toxicity score is 0.08687818818444444\n",
      "------\n",
      "corporate_titles average toxicity score is 0.059584539100000014\n",
      "------\n",
      "industrial_occupations average toxicity score is 0.10607839670588236\n",
      "------\n",
      "railway_industry_occupations average toxicity score is 0.09670814815\n"
     ]
    }
   ],
   "source": [
    "top_k_professions_groups = compute_toxicity_subgroups(generated_profession_dict, \"Professions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "TAqvI-_7hA4m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Race Groups\n",
      "------\n",
      "Asian_Americans average toxicity score is 0.09860196344333334\n",
      "------\n",
      "African_Americans average toxicity score is 0.09212006166842106\n",
      "------\n",
      "European_Americans average toxicity score is 0.09329618429857142\n",
      "------\n",
      "Hispanic_and_Latino_Americans average toxicity score is 0.12905940564285714\n"
     ]
    }
   ],
   "source": [
    "top_k_race_groups = compute_toxicity_subgroups(generated_race_dict, \"Race Groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we generate sentences with length 64 with top_k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_generated_gender_dict = create_dict_with_generated(gender_dict, 64, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_generated_political_dict = create_dict_with_generated(political_dict, 64, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_generated_religion_dict = create_dict_with_generated(religion_dict, 64, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_generated_profession_dict = create_dict_with_generated(profession_dict, 64, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 108, but ``max_length`` is set to 64. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 69, but ``max_length`` is set to 64. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 73, but ``max_length`` is set to 64. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    }
   ],
   "source": [
    "longer_generated_race_dict = create_dict_with_generated(race_dict, 64, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Gender Groups\n",
      "------\n",
      "American_actors average toxicity score is 0.1189462085090244\n",
      "------\n",
      "American_actresses average toxicity score is 0.11916088325732217\n"
     ]
    }
   ],
   "source": [
    "_ = compute_toxicity_subgroups(longer_generated_gender_dict, \"Gender Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Political Groups\n",
      "------\n",
      "left-wing average toxicity score is 0.17046700739999998\n",
      "------\n",
      "right-wing average toxicity score is 0.12050389989999999\n",
      "------\n",
      "communism average toxicity score is 0.18326958820666667\n",
      "------\n",
      "socialism average toxicity score is 0.16036001109999995\n",
      "------\n",
      "democracy average toxicity score is 0.09904319335714286\n",
      "------\n",
      "liberalism average toxicity score is 0.1687488687\n",
      "------\n",
      "populism average toxicity score is 0.1486334964\n",
      "------\n",
      "conservatism average toxicity score is 0.10777075795999999\n",
      "------\n",
      "nationalism average toxicity score is 0.16433051717204303\n",
      "------\n",
      "anarchism average toxicity score is 0.17283283697435897\n",
      "------\n",
      "capitalism average toxicity score is 0.12488422595000001\n",
      "------\n",
      "fascism average toxicity score is 0.29428960766666673\n"
     ]
    }
   ],
   "source": [
    "_ = compute_toxicity_subgroups(longer_generated_political_dict, \"Political Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Religions\n",
      "------\n",
      "judaism average toxicity score is 0.16803576635\n",
      "------\n",
      "christianity average toxicity score is 0.1423559789\n",
      "------\n",
      "islam average toxicity score is 0.25074383390000005\n",
      "------\n",
      "hinduism average toxicity score is 0.16687282409999998\n",
      "------\n",
      "buddhism average toxicity score is 0.1090488112333333\n",
      "------\n",
      "sikhism average toxicity score is 0.1215498956\n",
      "------\n",
      "atheism average toxicity score is 0.27185481800000005\n"
     ]
    }
   ],
   "source": [
    "_ = compute_toxicity_subgroups(longer_generated_religion_dict, \"Religions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Professions\n",
      "------\n",
      "metalworking_occupations average toxicity score is 0.09871533206923078\n",
      "------\n",
      "sewing_occupations average toxicity score is 0.11788614697941177\n",
      "------\n",
      "healthcare_occupations average toxicity score is 0.11944022063636363\n",
      "------\n",
      "computer_occupations average toxicity score is 0.0977792447\n",
      "------\n",
      "film_and_television_occupations average toxicity score is 0.11434371107843137\n",
      "------\n",
      "artistic_occupations average toxicity score is 0.10677433187444445\n",
      "------\n",
      "scientific_occupations average toxicity score is 0.10408003804705884\n",
      "------\n",
      "entertainer_occupations average toxicity score is 0.11197571856666667\n",
      "------\n",
      "dance_occupations average toxicity score is 0.09995991198723078\n",
      "------\n",
      "nursing_specialties average toxicity score is 0.09610305306666667\n",
      "------\n",
      "writing_occupations average toxicity score is 0.1041395341888889\n",
      "------\n",
      "professional_driver_types average toxicity score is 0.10801830255\n",
      "------\n",
      "engineering_branches average toxicity score is 0.093545514077813\n",
      "------\n",
      "mental_health_occupations average toxicity score is 0.0967168591\n",
      "------\n",
      "theatre_personnel average toxicity score is 0.09701789646470588\n",
      "------\n",
      "corporate_titles average toxicity score is 0.06330619109999999\n",
      "------\n",
      "industrial_occupations average toxicity score is 0.11419085851851853\n",
      "------\n",
      "railway_industry_occupations average toxicity score is 0.08615600133333333\n"
     ]
    }
   ],
   "source": [
    "_ = compute_toxicity_subgroups(longer_generated_profession_dict, \"Professions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Race Groups\n",
      "------\n",
      "Asian_Americans average toxicity score is 0.11753909084416668\n",
      "------\n",
      "African_Americans average toxicity score is 0.1070523838908\n",
      "------\n",
      "European_Americans average toxicity score is 0.1041409248282796\n",
      "------\n",
      "Hispanic_and_Latino_Americans average toxicity score is 0.1287183832\n"
     ]
    }
   ],
   "source": [
    "_ = compute_toxicity_subgroups(longer_generated_race_dict, \"Race Groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we generate sentences with length 32 with top_k = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_gender_dict_100 = create_dict_with_generated(gender_dict, 32, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 34, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 34, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    }
   ],
   "source": [
    "generated_political_dict_100 = create_dict_with_generated(political_dict, 32, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_religion_dict_100 = create_dict_with_generated(religion_dict, 32, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 34, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 44, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    }
   ],
   "source": [
    "generated_profession_dict_100 = create_dict_with_generated(profession_dict, 32, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 108, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 41, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 33, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 33, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 35, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 34, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 35, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 34, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 41, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 69, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 32, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 61, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 36, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 73, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 36, but ``max_length`` is set to 32. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    }
   ],
   "source": [
    "generated_race_dict_100 = create_dict_with_generated(race_dict, 32, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Gender Groups\n",
      "------\n",
      "American_actors average toxicity score is 0.11184316429999999\n",
      "------\n",
      "American_actresses average toxicity score is 0.11175216017562502\n"
     ]
    }
   ],
   "source": [
    "_ = compute_toxicity_subgroups(generated_gender_dict_100, \"Gender Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Political Groups\n",
      "------\n",
      "left-wing average toxicity score is 0.21502085049999997\n",
      "------\n",
      "right-wing average toxicity score is 0.11173686670588234\n",
      "------\n",
      "communism average toxicity score is 0.13124429375\n",
      "------\n",
      "socialism average toxicity score is 0.1354569864\n",
      "------\n",
      "democracy average toxicity score is 0.08000006218\n",
      "------\n",
      "liberalism average toxicity score is 0.1135513888\n",
      "------\n",
      "populism average toxicity score is 0.08738419389999999\n",
      "------\n",
      "conservatism average toxicity score is 0.14726760199999997\n",
      "------\n",
      "nationalism average toxicity score is 0.23931908423809525\n",
      "------\n",
      "anarchism average toxicity score is 0.14673130485714284\n",
      "------\n",
      "capitalism average toxicity score is 0.0753665769\n",
      "------\n",
      "fascism average toxicity score is 0.28878401949999993\n"
     ]
    }
   ],
   "source": [
    "_ = compute_toxicity_subgroups(generated_political_dict_100, \"Political Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Religions\n",
      "------\n",
      "judaism average toxicity score is 0.22393245744999998\n",
      "------\n",
      "christianity average toxicity score is 0.19834687906666665\n",
      "------\n",
      "islam average toxicity score is 0.28574313949999997\n",
      "------\n",
      "hinduism average toxicity score is 0.1528782258\n",
      "------\n",
      "buddhism average toxicity score is 0.1354753159\n",
      "------\n",
      "sikhism average toxicity score is 0.0853695713\n",
      "------\n",
      "atheism average toxicity score is 0.23834252200000003\n"
     ]
    }
   ],
   "source": [
    "_ = compute_toxicity_subgroups(generated_religion_dict_100, \"Religions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Professions\n",
      "------\n",
      "metalworking_occupations average toxicity score is 0.10774842068222222\n",
      "------\n",
      "sewing_occupations average toxicity score is 0.11301799873275863\n",
      "------\n",
      "healthcare_occupations average toxicity score is 0.09617164085897437\n",
      "------\n",
      "computer_occupations average toxicity score is 0.07524815901666668\n",
      "------\n",
      "film_and_television_occupations average toxicity score is 0.07557964715\n",
      "------\n",
      "artistic_occupations average toxicity score is 0.10463747431333334\n",
      "------\n",
      "scientific_occupations average toxicity score is 0.1023122215318182\n",
      "------\n",
      "entertainer_occupations average toxicity score is 0.09721022788333335\n",
      "------\n",
      "dance_occupations average toxicity score is 0.09273227261111112\n",
      "------\n",
      "nursing_specialties average toxicity score is 0.085471833375\n",
      "------\n",
      "writing_occupations average toxicity score is 0.08579422628333334\n",
      "------\n",
      "professional_driver_types average toxicity score is 0.1975378419\n",
      "------\n",
      "engineering_branches average toxicity score is 0.09057296847261903\n",
      "------\n",
      "mental_health_occupations average toxicity score is 0.09729303567999999\n",
      "------\n",
      "theatre_personnel average toxicity score is 0.09598077082916667\n",
      "------\n",
      "corporate_titles average toxicity score is 0.08877470955000001\n",
      "------\n",
      "industrial_occupations average toxicity score is 0.09291060314999999\n",
      "------\n",
      "railway_industry_occupations average toxicity score is 0.08345140464999999\n"
     ]
    }
   ],
   "source": [
    "_ = compute_toxicity_subgroups(generated_profession_dict_100, \"Professions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Race Groups\n",
      "------\n",
      "Asian_Americans average toxicity score is 0.10172814172833333\n",
      "------\n",
      "African_Americans average toxicity score is 0.10869538526799999\n",
      "------\n",
      "European_Americans average toxicity score is 0.09230612280689847\n",
      "------\n",
      "Hispanic_and_Latino_Americans average toxicity score is 0.1290260842\n"
     ]
    }
   ],
   "source": [
    "_ = compute_toxicity_subgroups(generated_race_dict_100, \"Race Groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we generate sentences with length 64 with top_k = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_generated_gender_dict_100 = create_dict_with_generated(gender_dict, 64, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_generated_political_dict_100 = create_dict_with_generated(political_dict, 64, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_generated_religion_dict_100 = create_dict_with_generated(religion_dict, 64, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_generated_profession_dict_100 = create_dict_with_generated(profession_dict, 64, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 108, but ``max_length`` is set to 64. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 69, but ``max_length`` is set to 64. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 73, but ``max_length`` is set to 64. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    }
   ],
   "source": [
    "longer_generated_race_dict_100 = create_dict_with_generated(race_dict, 64, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Gender Groups\n",
      "------\n",
      "American_actors average toxicity score is 0.12136788949500002\n",
      "------\n",
      "American_actresses average toxicity score is 0.135406198810625\n"
     ]
    }
   ],
   "source": [
    "_ = compute_toxicity_subgroups(longer_generated_gender_dict_100, \"Gender Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Political Groups\n",
      "------\n",
      "left-wing average toxicity score is 0.17611750425\n",
      "------\n",
      "right-wing average toxicity score is 0.1174695318\n",
      "------\n",
      "communism average toxicity score is 0.18319753619999998\n",
      "------\n",
      "socialism average toxicity score is 0.13771108836\n",
      "------\n",
      "democracy average toxicity score is 0.10526945179999998\n",
      "------\n",
      "liberalism average toxicity score is 0.17849241829999998\n",
      "------\n",
      "populism average toxicity score is 0.15577109819999999\n",
      "------\n",
      "conservatism average toxicity score is 0.09909958055\n",
      "------\n",
      "nationalism average toxicity score is 0.1802339738375\n",
      "------\n",
      "anarchism average toxicity score is 0.23491921970000001\n",
      "------\n",
      "capitalism average toxicity score is 0.15454058334999998\n",
      "------\n",
      "fascism average toxicity score is 0.28219633569999997\n"
     ]
    }
   ],
   "source": [
    "_ = compute_toxicity_subgroups(longer_generated_political_dict_100, \"Political Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Religions\n",
      "------\n",
      "judaism average toxicity score is 0.18717357569999998\n",
      "------\n",
      "christianity average toxicity score is 0.15588964656666662\n",
      "------\n",
      "islam average toxicity score is 0.33377527774999993\n",
      "------\n",
      "hinduism average toxicity score is 0.1013375924\n",
      "------\n",
      "buddhism average toxicity score is 0.10968651536666668\n",
      "------\n",
      "sikhism average toxicity score is 0.0879627537935\n",
      "------\n",
      "atheism average toxicity score is 0.23019791\n"
     ]
    }
   ],
   "source": [
    "_ = compute_toxicity_subgroups(longer_generated_religion_dict_100, \"Religions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Professions\n",
      "------\n",
      "metalworking_occupations average toxicity score is 0.11680720145454544\n",
      "------\n",
      "sewing_occupations average toxicity score is 0.12272413232857142\n",
      "------\n",
      "healthcare_occupations average toxicity score is 0.1043907853777778\n",
      "------\n",
      "computer_occupations average toxicity score is 0.07951381637499999\n",
      "------\n",
      "film_and_television_occupations average toxicity score is 0.15702913888888886\n",
      "------\n",
      "artistic_occupations average toxicity score is 0.11998041333750002\n",
      "------\n",
      "scientific_occupations average toxicity score is 0.10200579529084505\n",
      "------\n",
      "entertainer_occupations average toxicity score is 0.11454597251250001\n",
      "------\n",
      "dance_occupations average toxicity score is 0.10517230785363636\n",
      "------\n",
      "nursing_specialties average toxicity score is 0.11061291916\n",
      "------\n",
      "writing_occupations average toxicity score is 0.12320566723611111\n",
      "------\n",
      "professional_driver_types average toxicity score is 0.07856710269999999\n",
      "------\n",
      "engineering_branches average toxicity score is 0.10693139845201535\n",
      "------\n",
      "mental_health_occupations average toxicity score is 0.12454633703333334\n",
      "------\n",
      "theatre_personnel average toxicity score is 0.09613549018881119\n",
      "------\n",
      "corporate_titles average toxicity score is 0.07106513965\n",
      "------\n",
      "industrial_occupations average toxicity score is 0.10334026365000001\n",
      "------\n",
      "railway_industry_occupations average toxicity score is 0.10561355269999997\n"
     ]
    }
   ],
   "source": [
    "_ = compute_toxicity_subgroups(longer_generated_profession_dict_100, \"Professions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity metrics for Race Groups\n",
      "------\n",
      "Asian_Americans average toxicity score is 0.12096400194333333\n",
      "------\n",
      "African_Americans average toxicity score is 0.1304532295526738\n",
      "------\n",
      "European_Americans average toxicity score is 0.10384134281107216\n",
      "------\n",
      "Hispanic_and_Latino_Americans average toxicity score is 0.1834307663478261\n"
     ]
    }
   ],
   "source": [
    "_ = compute_toxicity_subgroups(longer_generated_race_dict_100, \"Race Groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-P Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use shorter sentence generation lengths w/top_p = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences_top_p(s, max_length, top_p):\n",
    "    inputs = tokenizer.encode(s, return_tensors='pt').to(model.device)\n",
    "    size = inputs.nelement()\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        do_sample=True, # Randomly sample from the logits instead of greedily picking next word with highest probability\n",
    "        top_p=top_p,    # Will select the smallest number of words in which its probabilities add up to top-p\n",
    "        top_k=0,                 \n",
    "        max_length=max_length,              # Generate for a maximum of 64 tokens\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    ).to(model.device)\n",
    "    return [tokenizer.decode(x) for x in outputs][0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_create_dict_with_generated(d, max_length, top_p):\n",
    "    generated_d = {}\n",
    "    for key, val in d.items():\n",
    "        # Create generations for each sentence\n",
    "        generated_sentences = [generate_sentences_top_p(sentence, max_length, top_p) for sentence in val if sentence != \"\"]\n",
    "        # Convert two-d array of generated sentences into 1d list\n",
    "        generated_d[key] = generated_sentences\n",
    "    return generated_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p_generated_gender_dict = top_p_create_dict_with_generated(gender_dict, 32, 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p_generated_political_dict = top_p_create_dict_with_generated(political_dict, 32, 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p_generated_religion_dict = top_p_create_dict_with_generated(religion_dict, 32, 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p_generated_profession_dict = top_p_create_dict_with_generated(profession_dict, 32, 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p_generated_race_dict = top_p_create_dict_with_generated(race_dict, 32, 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(top_p_generated_gender_dict, \"Gender Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(top_p_generated_political_dict, \"Political Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(top_p_generated_religion_dict, \"Religions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(top_p_generated_profession_dict, \"Professions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(top_p_generated_race_dict, \"Race Groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-P Sampling Variation 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use longer sentence generation lengths (64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_top_p_generated_gender_dict = top_p_create_dict_with_generated(gender_dict, 64, 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_top_p_generated_political_dict = top_p_create_dict_with_generated(political_dict, 64, 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_top_p_generated_religion_dict = top_p_create_dict_with_generated(religion_dict, 64, 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_top_p_generated_profession_dict = top_p_create_dict_with_generated(profession_dict, 64, 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_top_p_generated_race_dict = top_p_create_dict_with_generated(race_dict, 64, 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(longer_top_p_generated_gender_dict, \"Gender Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(longer_top_p_generated_political_dict, \"Political Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(longer_top_p_generated_religion_dict, \"Religions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(longer_top_p_generated_profession_dict, \"Professions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(longer_top_p_generated_race_dict, \"Race Groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use shorter sentence generation lengths w/top_p = 0.975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p_generated_gender_dict_up = top_p_create_dict_with_generated(gender_dict, 32, 0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p_generated_political_dict_up = top_p_create_dict_with_generated(political_dict, 32, 0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p_generated_religion_dict_up = top_p_create_dict_with_generated(religion_dict, 32, 0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p_generated_profession_dict_up = top_p_create_dict_with_generated(profession_dict, 32, 0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p_generated_race_dict_up = top_p_create_dict_with_generated(race_dict, 32, 0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(top_p_generated_gender_dict_up, \"Gender Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(top_p_generated_political_dict_up, \"Political Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(top_p_generated_religion_dict_up, \"Religions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(top_p_generated_profession_dict_up, \"Professions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(top_p_generated_race_dict_up, \"Race Groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use longer sentence generation lengths w/top_p = 0.975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_top_p_generated_gender_dict_up = top_p_create_dict_with_generated(gender_dict, 64, 0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_top_p_generated_political_dict_up = top_p_create_dict_with_generated(political_dict, 64, 0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_top_p_generated_religion_dict_up = top_p_create_dict_with_generated(religion_dict, 64, 0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_top_p_generated_profession_dict_up = top_p_create_dict_with_generated(profession_dict, 64, 0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 108, but ``max_length`` is set to 64. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 69, but ``max_length`` is set to 64. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 73, but ``max_length`` is set to 64. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    }
   ],
   "source": [
    "longer_top_p_generated_race_dict_up = top_p_create_dict_with_generated(race_dict, 64, 0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a binary pickle file \n",
    "f = open(\"longer_top_p_generated_race_dict_up.pkl\",\"wb\")\n",
    "\n",
    "# write the python object (dict) to pickle file\n",
    "pickle.dump(longer_top_p_generated_race_dict_up ,f)\n",
    "\n",
    "# close file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(longer_top_p_generated_gender_dict_up, \"Gender Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(longer_top_p_generated_political_dict_up, \"Political Groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(longer_top_p_generated_religion_dict_up, \"Religions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(longer_top_p_generated_profession_dict_up, \"Professions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_toxicity_subgroups(longer_top_p_generated_race_dict_up, \"Race Groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the GPT-2 Model so we need to initialize the tokenizer and our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below was adapted from this blog post: https://towardsdatascience.com/build-a-bidirectional-text-generator-with-xlnet-49d9d37b48a9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, it allows sentence generation using XLNet, and the way the sentences are generated are using a Beam Search method. Here we will just play around using the top_k parameters. More details about these functions can be found at the source blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a binary pickle file \n",
    "f = open(\"longest_religion.pkl\",\"wb\")\n",
    "\n",
    "# write the python object (dict) to pickle file\n",
    "pickle.dump(long_generated_religion_dict ,f)\n",
    "\n",
    "# close file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_read = open(\"longest_gender.pkl\", \"rb\")\n",
    "\n",
    "long_generated_political_dict = pickle.load(file_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "XLM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
